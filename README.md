# InsightAI ğŸ¤–ğŸ“šğŸ–¼ï¸

A multimodal Retrieval-Augmented Generation (RAG) chatbot that can understand and answer questions about **documents, images, and YouTube videos**, with optional **web search** and **UI generation** tools.

- **Check it out**: https://insight-ai-theta.vercel.app/ ğŸš€


---

## Overview ğŸ§­

This project lets you:

- ğŸ“„ Upload PDFs and images.
- ğŸ“º Ingest YouTube videos via their subtitles.
- ğŸ’¬ Ask natural language questions about your content.
- ğŸ” Retrieve and combine relevant text and image information via vector search.
- ğŸ§° Optionally call tools (web search, UI generator) when the model decides they are helpful.

The goal is to provide a lightweight but powerful demo of a multimodal RAG system built with modern tooling on both the frontend and backend.

---

## Features âœ¨

- **Document upload (PDFs & images)** ğŸ“  
- PDFs: text is extracted, chunked, embedded, and stored in Pinecone.  
- Images: captioned using an OpenAI vision model; captions are embedded and stored for retrieval.

- **InsightAI** ğŸ§   
- Uses the same text embedding space for:  
- Document text chunks  
- Image captions  
- Answers questions grounded in retrieved context (text + images).

- **YouTube ingestion** ğŸ¬  
- Accepts a YouTube URL.  
- Fetches the transcript (if available), indexes it, and generates a concise summary.  
- Lets you chat about the video as if it were a document.

- **Web search tool** ğŸŒ  
- Uses Tavily API to search the web and summarize results when local context is not enough.

- **UI generation tool** ğŸ¨  
- Given a textual spec, asks an LLM to generate a small React/TSX component.

- **Clean chat UI** ğŸ’¬  
- Document list with active selection.  
- Chat history with user and bot messages.  
- Light/dark theme toggle ğŸŒğŸŒ™.

---

## Tech Stack (Brief) ğŸ› ï¸

- **Frontend** âš›ï¸  
- Next.js (App Router)  
- React + TypeScript  
- Axios  
- Deployed on Vercel  

- **Backend** ğŸ  
- FastAPI + Uvicorn  
- OpenAI API (chat, vision, embeddings)  
- Pinecone (vector database)  
- PyPDF2, PyMuPDF (PDF handling)  
- youtube-transcript-api (YouTube subtitles)  
- Deployed on Render  

---

## Setup Instructions âš™ï¸

### 1. Prerequisites

- Python 3.11+ ğŸ
- Node.js (LTS) + npm ğŸ§©
- A Pinecone account and two indexes:
  - Text index (dimension **3072**)
  - Image index (dimension **3072**)
- API keys:
  - OpenAI
  - Tavily
  - Pinecone

---

### 2. Clone the Repository

```bash
git clone https://github.com/<your-username>/multimodal-rag.git
cd InsightAI
```

---

### 3. Backend Setup (FastAPI) ğŸ§ 

From the project root:

```bash
cd backend
python -m venv venv
# PowerShell
.\venv\Scripts\Activate.ps1
# or Command Prompt
venv\Scripts\activate.bat
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Create a `.env` file in `backend/`:

```env
OPENAI_API_KEY=your_openai_key
TAVILY_API_KEY=your_tavily_key

PINECONE_API_KEY=your_pinecone_key
PINECONE_TEXT_INDEX=your_text_index_name       # dimension 3072
PINECONE_IMAGE_INDEX=your_image_index_name     # dimension 3072

APP_ENV=development
```



Run the backend locally:

```bash
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

- API root: `http://localhost:8000`
- Interactive docs: `http://localhost:8000/docs`

---

### 4. Frontend Setup (Next.js) ğŸ’»

In a new terminal window:

```bash
cd frontend
npm install
```

Create `frontend/.env.local`:

```env
NEXT_PUBLIC_API_URL=http://localhost:8000
```

Start the development server:

```bash
npm run dev
```

- Frontend: `http://localhost:3000`

You can now:

1. Open `http://localhost:3000`.
2. Upload a PDF or image.
3. Ask questions about your uploaded content.
4. Try YouTube ingestion and web search modes.

---
